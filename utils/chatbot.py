from litellm import completion
import os
import streamlit as st
import random

# ---------------------------------------------------------
# ðŸ›‘ MODE TEST : Mettez True pour Ã©conomiser vos tokens !
# Mettez False pour la dÃ©mo finale.
MOCK_MODE = True
# ---------------------------------------------------------

class EcoAssistant:
    def __init__(self):
        # ðŸ“‹ LISTE DE PRIORITÃ‰ DES MODÃˆLES (Mise Ã  jour 12/2025)
        self.models_priority = [
            "groq/llama-3.1-8b-instant",                # Groq (Llama 3.1)
            "gemini/gemini-2.5-flash-lite",                  # Gemini 1.5 Flash
            "huggingface/HuggingFaceH4/zephyr-7b-beta"  # Hugging Face (Zephyr)
        ]

    # ðŸ‘‡ C'EST ICI QUE SE TROUVAIT L'ERREUR (Il manquait custom_priority=None)
    def _call_llm_with_fallback(self, messages, custom_priority=None):
        """
        Tente d'appeler les modÃ¨les en cascade.
        Si MOCK_MODE est activÃ©, renvoie une rÃ©ponse simulÃ©e instantanÃ©ment.
        """
        
        # --- ðŸ›‘ INTERCEPTION POUR LE MODE TEST ---
        if MOCK_MODE:
            # On simule une petite latence ou une rÃ©ponse immÃ©diate
            return (
                "ðŸ¤– **[MODE SIMULATION]**\n\n"
                "J'Ã©conomise vos tokens ! ðŸ’°\n"
                "Si l'IA Ã©tait active, elle aurait analysÃ© votre demande avec pertinence.\n\n"
                "Voici une rÃ©ponse type : *'Le train est l'option la plus Ã©cologique pour ce trajet, "
                "Ã©mettant 50x moins de CO2 que l'avion.'*"
            )
        # -----------------------------------------

        # On utilise la liste personnalisÃ©e si fournie, sinon celle par dÃ©faut
        priority_list = custom_priority if custom_priority else self.models_priority
        
        errors = []
        
        for model in priority_list:
            try:
                # Appel via LiteLLM
                response = completion(
                    model=model,
                    messages=messages
                )
                return response.choices[0].message.content
                
            except Exception as e:
                error_msg = f"âš ï¸ Ã‰chec sur {model} : {str(e)}"
                print(error_msg)
                errors.append(error_msg)
                continue
        
        return f"âŒ Service indisponible. Tous les modÃ¨les ont Ã©chouÃ©.\nDÃ©tails : {'; '.join(errors)}"

    def analyze_trip(self, start, end, df_results):
        """Analyse du trajet (Force Gemini en premier car meilleur en raisonnement)."""
        
        # En mode MOCK, on renvoie une fausse analyse statique
        if MOCK_MODE:
            return (
                "### ðŸŒ± Analyse Rapide (Simulation)\n"
                f"Pour aller de **{start}** Ã  **{end}** :\n\n"
                "- ðŸš„ **Le Train** est le grand gagnant (rapide et propre).\n"
                "- ðŸš— **La Voiture** Ã©met beaucoup plus, surtout si vous Ãªtes seul.\n"
                "- âœˆï¸ **L'Avion** est Ã  Ã©viter pour cette distance.\n\n"
                "> *Note : DÃ©sactivez MOCK_MODE dans le code pour avoir la vraie analyse IA.*"
            )
        
        data_context = df_results.to_string()
        
        prompt = f"""
        Tu es un expert en mobilitÃ© Ã©cologique. Analyse ce trajet : {start} -> {end}.
        Voici les donnÃ©es calculÃ©es :
        {data_context}

        Tes missions :
        1. Compare le TRAIN vs VOITURE vs AVION de maniÃ¨re percutante.
        2. Donne une Ã©quivalence concrÃ¨te pour le CO2 Ã©conomisÃ© par le train (ex: nombre de repas vÃ©gÃ©tariens, jours de chauffage...).
        3. Sois encourageant et pÃ©dagogique.
        """
        messages = [{"role": "user", "content": prompt}]
        
        # Ordre spÃ©cifique pour l'analyse : Gemini d'abord
        analysis_priority = [
            "gemini/gemini-2.5-flash-lite",
            "groq/llama-3.1-8b-instant",
            "huggingface/HuggingFaceH4/zephyr-7b-beta"
        ]
        return self._call_llm_with_fallback(messages, custom_priority=analysis_priority)

    def chat(self, user_question, context_str="", use_groq=True):
        """Chatbot interactif."""
        messages = [
            {"role": "system", "content": f"Tu es EcoBot, un assistant spÃ©cialisÃ© dans l'impact carbone des transports. Contexte actuel : {context_str}"},
            {"role": "user", "content": user_question}
        ]
        
        if use_groq:
            # Ordre standard : Groq -> Gemini -> HF
            return self._call_llm_with_fallback(messages)
        else:
            # Ordre inversÃ© : Gemini -> Groq -> HF
            gemini_first = [
                "gemini/gemini-2.5-flash-lite",
                "groq/llama-3.1-8b-instant",
                "huggingface/HuggingFaceH4/zephyr-7b-beta"
            ]
            return self._call_llm_with_fallback(messages, custom_priority=gemini_first)